{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Allen-Cahn 方程求解\n",
    "\n",
    "## 环境安装与依赖\n",
    "\n",
    "本案例使用 **MindSpore >= 2.0.0** 运行，支持函数式编程接口（如 `mindspore.jit`、`mindspore.jit_class`、`mindspore.data_sink`），以实现高效的神经网络训练。安装方法请参考 [MindSpore 官方安装指南](https://www.mindspore.cn/install)。\n",
    "\n",
    "此外，需要安装 **MindFlow >= 0.1.0**，这是 MindSpore 的流体模拟套件，提供 PDE 求解和 PINNs 方法的支持。如果尚未安装，请根据硬件环境（GPU 或 NPU）选择合适的版本并执行以下代码安装。\n",
    "\n",
    "本案例在 GPU 环境下运行，若使用 NPU，请注释掉 GPU 相关安装代码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mindflow_version = \"0.1.0\"  # update if needed\n",
    "# GPU Comment out the following code if you are using NPU.\n",
    "!pip uninstall -y mindflow-gpu\n",
    "!pip install mindflow-gpu==$mindflow_version\n",
    "\n",
    "# NPU Uncomment if needed.\n",
    "# !pip uninstall -y mindflow-ascend\n",
    "# !pip install mindflow-ascend==$mindflow_version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 概述\n",
    "\n",
    "Allen-Cahn 方程是一种非线性偏微分方程（PDE），以 John W. Cahn 和 Sam Allen 命名，广泛应用于相场模型，描述多组分合金系统中的相分离过程，如有序-无序转变。其形式为一个反应-扩散方程，刻画标量状态变量\n",
    "\n",
    "$$ u(x, t) $$\n",
    "\n",
    "在时空中的演化。本案例利用 **MindFlow** 流体模拟套件，基于物理驱动神经网络（PINNs）方法求解 Allen-Cahn 方程。\n",
    "\n",
    "传统数值方法（如有限元法 FEM 和有限差分法 FDM）在求解 PDE 时面临建模复杂、网格划分繁琐、计算成本高等问题。PINNs 结合神经网络的通用逼近能力和自动微分技术，通过最小化 PDE 残差及边界/初始条件损失，直接学习解的映射\n",
    "\n",
    "$$ (x, t) \\mapsto u(x, t) $$\n",
    "\n",
    "无需显式网格划分，显著提高效率。\n",
    "\n",
    "本案例展示了如何使用 PINNs 求解 Allen-Cahn 方程，包括数据集生成、模型构建、损失函数定义、训练优化及结果可视化，为非线性 PDE 求解提供了一种高效的替代方案。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题描述\n",
    "\n",
    "Allen-Cahn 方程的数学形式为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial u}{\\partial t} = d \\frac{\\partial^2 u}{\\partial x^2} + 5(u - u^3), \\quad x \\in [-1, 1], \\quad t \\in [0, 1], \\quad d = 0.001\n",
    "$$\n",
    "\n",
    "其中：\n",
    "- $ u(x, t) $ 是状态变量，表示相场量。\n",
    "- $ d = 0.001 $ 是扩散系数。\n",
    "- $ f(u) = 5(u - u^3) $ 是非线性反应项，驱动相分离行为。\n",
    "\n",
    "边界条件（Dirichlet）：\n",
    "\n",
    "$$\n",
    "u(-1, t) = -1, \\quad u(1, t) = -1, \\quad t \\in [0, 1]\n",
    "$$\n",
    "\n",
    "初始条件：\n",
    "\n",
    "$$\n",
    "u(x, 0) = x^2 \\cos(\\pi x), \\quad x \\in [-1, 1]\n",
    "$$\n",
    "\n",
    "本案例的目标是通过 PINNs 方法，训练神经网络学习映射\n",
    "\n",
    "$$(x, t) \\mapsto u(x, t)$$\n",
    "\n",
    "使之满足 Allen-Cahn 方程及其边界/初始条件。PINNs 通过构造损失函数（基于 PDE 残差），利用自动微分计算导数，无需传统网格划分，直接优化神经网络参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 技术路径\n",
    "\n",
    "MindFlow 求解 Allen-Cahn 方程的流程基于 PINNs 方法，具体步骤如下：\n",
    "\n",
    "1. **创建数据集**：根据定义域随机采样训练数据，并加载测试数据集用于评估。\n",
    "2. **构建模型**：使用多尺度全连接神经网络，支持硬约束边界/初始条件。\n",
    "3. **优化器**：采用 Adam 优化器，设置学习率和衰减策略，优化网络参数。\n",
    "4. **定义 Allen-Cahn 问题**：通过 `AllenCahn` 类定义 PDE、边界/初始条件和损失函数，利用 SymPy 和 MindSpore 实现自动微分和损失计算。\n",
    "5. **模型训练**：基于 PDE 残差损失进行训练，定期评估 L2 误差，保存最优模型和检查点。\n",
    "6. **模型推理与可视化**：使用训练好的模型对定义域进行推理，生成 $ u(x, t) $ 的时空分布，并可视化结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import mindspore\n",
    "from mindspore import context, nn, ops, Tensor, jit, set_seed\n",
    "from mindspore import dtype as mstype\n",
    "from mindspore import load_checkpoint, load_param_into_net, save_checkpoint\n",
    "from mindflow.pde import PDEWithLoss, sympy_to_mindspore\n",
    "from mindflow.utils import load_yaml_config\n",
    "from mindflow.loss import get_loss_metric\n",
    "\n",
    "from sympy import diff, symbols, Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依赖包与源代码\n",
    "\n",
    "上述代码导入了求解 Allen-Cahn 方程所需的 MindSpore 和 MindFlow 核心模块，以及 SymPy 用于符号计算。[src](./src/) 包包含自定义的模型和工具函数，包括：\n",
    "\n",
    "- `MultiScaleFCSequentialOutputTransform`：多尺度全连接神经网络，支持输出变换。\n",
    "- `create_training_dataset` 和 `create_test_dataset`：生成训练和测试数据集。\n",
    "- `visual` 和 `calculate_l2_error`：可视化结果和计算 L2 误差。\n",
    "\n",
    "这些模块共同支持 PINNs 方法的实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import MultiScaleFCSequentialOutputTransform\n",
    "from src import create_training_dataset, create_test_dataset, visual, calculate_l2_error\n",
    "\n",
    "# 设置随机种子以确保结果可重现\n",
    "set_seed(123456)\n",
    "np.random.seed(123456)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置运行上下文，使用 GPU 训练\n",
    "context.set_context(mode=context.GRAPH_MODE, device_target=\"GPU\", device_id=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从configs文件夹下导入配置文件\n",
    "\n",
    "注：配置文件默认为训练第一个数据集的配置，配置文件当中默认训练为 5000 轮，训练第二个数据集请将 test_dataset_path 改为 \"../dataset/2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载配置文件\n",
    "config = load_yaml_config('./configs/allen_cahn_cfg.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据集\n",
    "\n",
    "训练数据集通过随机采样生成，包括：\n",
    "- **定义域内部点**：8192 个点，用于计算 PDE 残差损失。\n",
    "- **边界点**：800 个点 $ x = \\pm 1 $ 用于边界条件。\n",
    "- **初始点**：400 个点 $ t = 0 $ 用于初始条件。\n",
    "\n",
    "采样方法为均匀随机采样（`uniform`），确保点分布覆盖整个时空定义域。测试数据集从指定路径加载，用于评估模型精度（L2 误差）。\n",
    "\n",
    "数据集目录：[allen_cahn/dataset](./allen_cahn/dataset)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建训练数据集\n",
    "ac_train_dataset = create_training_dataset(config)\n",
    "train_dataset = ac_train_dataset.create_dataset(batch_size=config[\"train_batch_size\"],\n",
    "                                                shuffle=True,\n",
    "                                                prebatched_data=True,\n",
    "                                                drop_remainder=True)\n",
    "# 创建测试数据集\n",
    "inputs, label = create_test_dataset(config[\"test_dataset_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建模型\n",
    "\n",
    "本案例使用 `MultiScaleFCSequentialOutputTransform` 或配置文件中的 `MLP_with_Residual` 模型，具体配置如下：\n",
    "- **结构**：6 层全连接网络，每层 128 个神经元。\n",
    "- **激活函数**：`tanh`，适合平滑非线性变换。\n",
    "- **残差连接**：启用，增强深层网络的训练稳定性。\n",
    "- **输入/输出**：输入为 $ (x, t) $ ，输出为 $ u(x, t) $。\n",
    "\n",
    "若启用检查点加载（`load_ckpt=True`），模型将从指定路径恢复预训练参数，继续训练或推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义模型\n",
    "model = MultiScaleFCSequentialOutputTransform(in_channels=config[\"model\"][\"in_channels\"],\n",
    "                                              out_channels=config[\"model\"][\"out_channels\"],\n",
    "                                              layers=config[\"model\"][\"layers\"],\n",
    "                                              neurons=config[\"model\"][\"neurons\"],\n",
    "                                              residual=config[\"model\"][\"residual\"],\n",
    "                                              act=config[\"model\"][\"activation\"],\n",
    "                                              num_scales=1)\n",
    "# 加载 checkpoint\n",
    "if config[\"load_ckpt\"]:\n",
    "    param_dict = load_checkpoint(config[\"load_ckpt_path\"])\n",
    "    load_param_into_net(model, param_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器\n",
    "\n",
    "优化器采用 Adam 算法，初始学习率从配置文件中读取（默认 0.0001）。支持学习率预热（`warmup_epochs`）和衰减（`gamma`），以提高训练稳定性和收敛速度。Adam 优化器管理模型的所有可训练参数，通过最小化损失函数更新网络权重。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义优化器\n",
    "optimizer = nn.Adam(model.trainable_params(),\n",
    "                    config[\"optimizer\"][\"initial_lr\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义模型\n",
    "\n",
    "`AllenCahn` 类继承自 `PDEWithLoss`，用于定义 Allen-Cahn 方程及其损失函数。主要功能包括：\n",
    "- **PDE 定义**：通过 SymPy 定义方程 $u_t - 0.001 u_{xx} - 5(u - u^3) = 0$ 并转换为 MindSpore 计算节点。\n",
    "- **输出变换**：通过 $u(x, t) = x^2 \\cos(\\pi x) + t (1 - x^2) \\cdot \\text{out}$ 硬编码边界条件 $u(\\pm 1, t) = -1$ 和初始条件 $u(x, 0) = x^2 \\cos(\\pi x)$\n",
    "- **损失计算**：仅计算 PDE 残差损失（均方误差），无需显式边界/初始条件损失。\n",
    "\n",
    "这种硬约束策略简化了损失函数设计，提高训练效率。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AllenCahn(PDEWithLoss):\n",
    "    \"\"\"\n",
    "    Allen-Cahn方程的PDE损失类，用于定义方程并计算物理驱动的神经网络损失。\n",
    "    继承自PDEWithLoss，通过SymPy定义PDE并转换为MindSpore计算节点，支持输出变换以满足边界条件。\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, loss_fn=\"mse\"):\n",
    "        \"\"\"\n",
    "        初始化Allen-Cahn方程的PDE模型。\n",
    "        参数：\n",
    "            model: 神经网络模型。\n",
    "            loss_fn: 损失函数，默认为均方误差（mse），可传入字符串或自定义函数。\n",
    "        \"\"\"\n",
    "        # 定义符号变量 x（空间）和 t（时间），以及函数 u(x, t)\n",
    "        self.x, self.t = symbols(\"x t\")\n",
    "        self.u = Function(\"u\")(self.x, self.t)\n",
    "        self.in_vars = [self.x, self.t]\n",
    "        self.out_vars = [self.u]\n",
    "        # 根据输入设置损失函数，字符串则使用预定义损失函数\n",
    "        if isinstance(loss_fn, str):\n",
    "            self.loss_fn = get_loss_metric(loss_fn)\n",
    "        else:\n",
    "            self.loss_fn = loss_fn\n",
    "        # 将PDE方程转换为MindSpore计算节点\n",
    "        self.pde_nodes = sympy_to_mindspore(\n",
    "            self.pde(), self.in_vars, self.out_vars)\n",
    "        # 设置模型的输出变换函数以满足边界条件\n",
    "        model.set_output_transform(self.output_transform)\n",
    "        super(AllenCahn, self).__init__(model, self.in_vars, self.out_vars)\n",
    "\n",
    "    def output_transform(self, x, out):\n",
    "        \"\"\"\n",
    "        输出变换函数，调整神经网络输出以满足Allen-Cahn方程的边界条件。\n",
    "        参数：\n",
    "            x: 输入张量，包含空间坐标 x 和时间 t。\n",
    "            out: 神经网络的原始输出。\n",
    "        返回：\n",
    "            调整后的输出，结合边界条件（如 x=±1 时值为零）。\n",
    "        \"\"\"\n",
    "        return x[:, 0:1] ** 2 * ops.cos(np.pi * x[:, 0:1]) + x[:, 1:2] * (1 - x[:, 0:1] ** 2) * out\n",
    "\n",
    "    def force_function(self, u):\n",
    "        \"\"\"\n",
    "        定义Allen-Cahn方程的非线性力项。\n",
    "        参数：\n",
    "            u: 函数值 u(x, t)。\n",
    "        返回：\n",
    "            非线性项 5 * (u - u^3)。\n",
    "        \"\"\"\n",
    "        return 5 * (u - u ** 3)\n",
    "\n",
    "    def pde(self):\n",
    "        \"\"\"\n",
    "        定义Allen-Cahn偏微分方程。\n",
    "        返回：\n",
    "            包含PDE损失的字典，形式为 u_t - d * u_xx - f(u)，其中 d=0.001。\n",
    "        \"\"\"\n",
    "        d = 0.001\n",
    "        loss_1 = (\n",
    "            self.u.diff(self.t)  # u 对时间 t 的一阶导数\n",
    "            - d * diff(self.u, (self.x, 2))  # 负 d 乘以 u 对 x 的二阶导数\n",
    "            - self.force_function(self.u)  # 减去非线性力项\n",
    "        )\n",
    "        return {\"loss_1\": loss_1}\n",
    "\n",
    "    def get_loss(self, pde_data):\n",
    "        \"\"\"\n",
    "        计算PDE残差损失。\n",
    "        参数：\n",
    "            pde_data: 输入数据，包含空间和时间坐标。\n",
    "        返回：\n",
    "            PDE损失值，基于PDE残差与零的均方误差。\n",
    "        \"\"\"\n",
    "        pde_res = ops.Concat(1)(self.parse_node(\n",
    "            self.pde_nodes, inputs=pde_data))  # 拼接PDE节点计算结果\n",
    "        pde_loss = self.loss_fn(\n",
    "            pde_res, Tensor(np.array([0.0]).astype(\n",
    "                np.float32), mstype.float32)  # 计算与零的损失\n",
    "        )\n",
    "\n",
    "        return pde_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型训练\n",
    "\n",
    "训练过程利用 **MindSpore >= 2.0.0** 的函数式编程范式，通过以下步骤实现：\n",
    "- **前向传播**：通过 `forward_fn` 计算 PDE 残差损失。\n",
    "- **初始化问题**：创建 `AllenCahn` 实例，定义 PDE 和损失函数。\n",
    "- **梯度计算**：使用 `ops.value_and_grad` 获取损失及其梯度。\n",
    "- **加速训练**：采用 JIT 编译（`@jit`）和 `data_sink` 优化数据流，提升训练效率。\n",
    "- **训练循环**：执行 epochs 轮训练（由配置文件指定），每批次 400 个采样点，定期评估 L2 误差，保存最优模型和检查点。\n",
    "\n",
    "训练仅优化 PDE 残差损失，边界和初始条件通过 `output_transform` 硬编码，无需显式损失项。训练过程记录每个 epoch 的损失和耗时，保存最优模型（`ac-optimal.ckpt`）和定期检查点（`ac-<epoch>.ckpt`）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    '''训练和评估神经网络，用于求解 Allen-Cahn 方程'''\n",
    "    # 初始化 Allen-Cahn 问题，传入神经网络模型\n",
    "    problem = AllenCahn(model)\n",
    "    \n",
    "    # 初始化损失缩放器\n",
    "    loss_scaler = None\n",
    "\n",
    "    # 计算 PDE 残差损失\n",
    "    def forward_fn(pde_data):\n",
    "        # 调用 AllenCahn 类的 get_loss 方法，计算 PDE 残差损失\n",
    "        loss = problem.get_loss(pde_data)\n",
    "        return loss\n",
    "\n",
    "    # 定义梯度计算函数，基于 forward_fn 获取损失值及其梯度\n",
    "    grad_fn = ops.value_and_grad(\n",
    "        forward_fn,            # 前向传播函数\n",
    "        None,                 # 不指定特定参数（默认对所有参数求梯度）\n",
    "        optimizer.parameters, # 优化器管理的神经网络参数\n",
    "        has_aux=False         # 不返回辅助输出，仅返回损失和梯度\n",
    "    )\n",
    "\n",
    "    # 使用 JIT（即时编译）加速训练过程\n",
    "    @jit\n",
    "    def train_step(pde_data):\n",
    "        loss, grads = grad_fn(pde_data)\n",
    "        loss = ops.depend(loss, optimizer(grads))\n",
    "        return loss\n",
    "    \n",
    "    # 跟踪最优模型\n",
    "    min_loss = math.inf\n",
    "\n",
    "    epochs = config[\"train_epochs\"]\n",
    "    steps_per_epochs = train_dataset.get_dataset_size()\n",
    "    # 使用 data_sink 加速数据处理，将 train_step 绑定到训练数据集\n",
    "    sink_process = mindspore.data_sink(train_step, train_dataset, sink_size=1)\n",
    "    \n",
    "    # 开始训练循环\n",
    "    for epoch in range(1, 1 + epochs):\n",
    "        time_beg = time.time()\n",
    "        model.set_train(True)\n",
    "        for _ in range(steps_per_epochs):\n",
    "            step_train_loss = sink_process()\n",
    "        print(\n",
    "            f\"epoch: {epoch} train loss: {step_train_loss} epoch time: {(time.time() - time_beg)*1000 :.3f} ms\")\n",
    "        \n",
    "        # 设置模型为评估模式（禁用 dropout 等）\n",
    "        model.set_train(False)  \n",
    "        if epoch % config[\"eval_interval_epochs\"] == 0:\n",
    "            # 计算模型预测与真实解的 L2 误差\n",
    "            calculate_l2_error(model, inputs, label, config[\"train_batch_size\"])\n",
    "        \n",
    "        # 保存最优模型（基于最小损失）\n",
    "        if config[\"save_ckpt\"] and step_train_loss < min_loss:\n",
    "            min_loss = step_train_loss\n",
    "            ckpt_name = \"ac-optimal.ckpt\"\n",
    "            # 保存最优模型检查点\n",
    "            save_checkpoint(model, os.path.join(config[\"save_ckpt_path\"], ckpt_name))\n",
    "        \n",
    "        # 按配置的保存间隔保存检查点\n",
    "        if epoch % config[\"save_checkpoint_epochs\"] == 0 and config[\"save_ckpt\"]:\n",
    "            if not os.path.exists(os.path.abspath(config[\"save_ckpt_path\"])):\n",
    "                config[\"save_ckpt_path\"] = os.path.abspath(\"./ckpt\")\n",
    "            ckpt_name = \"ac-{}.ckpt\".format(epoch)\n",
    "            save_checkpoint(model, os.path.join(config[\"save_ckpt_path\"], ckpt_name))\n",
    "            print(os.path.join(config[\"save_ckpt_path\"], ckpt_name))\n",
    "    print(f'final min_loss: {min_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "train()\n",
    "print(\"End-to-End total time: {} s\".format(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理与可视化\n",
    "\n",
    "训练完成后，加载最优模型进行推理，预测整个定义域的解。\n",
    "可视化结果包括：\n",
    "- **时空分布图**：展示 $ u(x, t) $ 在整个定义域的分布，颜色表示 $ u $ 值。\n",
    "- **时间截面图**：在特定时间点绘制 $ u(x) $ 曲线，反映相场演化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization\n",
    "if config[\"save_ckpt\"]:\n",
    "    optimal_param = load_checkpoint(os.path.join(config[\"save_ckpt_path\"], \"ac-optimal.ckpt\"))\n",
    "    load_param_into_net(model, optimal_param)\n",
    "    \n",
    "epochs = config[\"train_epochs\"]\n",
    "save_path = f'images/gpu_result_dataset_{config[\"test_dataset_path\"].split(\"/\")[-1]}.jpg'\n",
    "visual(model, epochs=epochs, resolution=config[\"visual_resolution\"],path=save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据集一可视化\n",
    "![dataset_1](./images/gpu_result_dataset_1.jpg)\n",
    "\n",
    "### 数据集二可视化\n",
    "\n",
    "![dataset_2](./images/gpu_result_dataset_2.jpg)\n",
    "\n",
    "### 分析\n",
    "\n",
    "* **时空分布图** $u(x, t)$ 在 $x = \\pm 1$ 始终为 $-1$，中心区域随 $t$ 增加形成 $u \\approx 1$ 并逐渐变窄，符合 Allen-Cahn 方程的**相分离特性**(系统自发分裂为 $u \\approx \\pm 1$ 的稳定相)。\n",
    "\n",
    "* **时间截面图** $u(x)$ 从宽峰逐步变窄，中心 $u \\approx 1$，两侧 $u \\approx -1$，反映出动态演化过程，即非线性项驱动界面向稳定态收敛。\n",
    "\n",
    "* 图像特征与 Allen-Cahn 方程的理论行为高度一致。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MindSpore",
   "language": "python",
   "name": "mindspore"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
